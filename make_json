#!/usr/bin/env python
# -*- coding: utf8 -*-

import urllib

import Levenshtein
import lxml.html

def correct(name):
    # Correct the spelling of cafe names.
    #
    # Pattern                => Correction
    substitution_table = (
    (u"AHO-kafeen",             u"AHO-kafeen"),
    (u"Annas Spiseri",          u"Annas spiseri"),
    (u"Forskningsveien",        u"Forskningsveien kafé"),
    (u"Frederikke kafe",        u"Frederikke kafé"),
    (u"Hannas spiseri",         u"Hannas spiseri"),
    (u"InforMATeket",           u"InforMATeket"),
    (u"Informatikkafeen",       u"Informatikkafeen"),
    (u"Kafe Helga",             u"Kafé Helga"),
    (u"Kafe Nova",              u"Kafé Nova"),
    (u"Kafe Ole",               u"Kafé Ole"),
    (u"Kafe Seilduken",         u"Kafé Seilduken"),
    (u"Musikkhøgskolens kafe",  u"Musikkhøgskolens kafé"),
    (u"NIH",                    u"Idrettshøgskolens kafé"),
    (u"Odontologi",             u"Odontologikafeen"),
    (u"Preklinisk",             u"Preklinisk kafé"),
    (u"Spiseriet P35",          u"Spiseriet P35"),
    (u"SV",                     u"SV-kafeen"),
    (u"Vetrinær",               u"Veterinærhøgskolens kafé"),
    )

    guesses = [(Levenshtein.distance(name, row[0]), row[1])
                for row in substitution_table]

    score, correction = min(guesses)

    assert score < 5

    return correction

def parse_index():
    url = "http://www.siofiler.no/Kafe/"
    htm = urllib.urlopen(url).read()
    doc = lxml.html.fromstring(htm)
    anchors = doc.findall("body/table/tr/td/a")

    return [url + a.text for a in anchors if a.text.startswith("meny")]

def parse_url(url):
    htm = urllib.urlopen(url).read()
    doc = lxml.html.fromstring(htm)
    tables = doc.findall("body/table")

    return map(parse_table, tables)

def parse_table(table):
    rows = table.findall("tr")
    week = int(rows[0].findall("td/p/span/strong")[0].text.split()[1])

    return {"week": week, "cafes": map(parse_row, rows[1:])}

def parse_row(row):
    cells = row.findall("td")
    name = unicode(cells[0].findall("p/span/strong")[0].text)
    menu = map(parse_cell, cells[1:])
    menu += (7 - len(menu)) * [[]]

    return {"name": correct(name), "menu": menu}

def parse_cell(cell):
    dishes = []
    for d in [unicode(i.text) for i in cell.findall("p//*")]:
        if d == "None" or not len(d) > 3:
            continue

        dishes.append(d)

    return dishes

if __name__ == "__main__":
    import json
    import sys

    import jsonschema

    if len(sys.argv) != 2:
        sys.stderr.write("usage: %s output_directory\n" % sys.argv[0])
        exit(1)

    schema = json.load(urllib.urlopen("http://www.dagensmiddag.net/menyer/schema.json"))

    for menu in parse_url(max(parse_index())):
        jsonschema.validate(menu, schema)

        path = "%s/2013-%02d.json" % (sys.argv[1], menu["week"])

        f = open(path, "w")
        json.dump(menu, f)
        f.close()
